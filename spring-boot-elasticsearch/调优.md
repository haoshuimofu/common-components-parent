#写
总结（可能的最佳实践）
通过解决这几个问题，可以对ES集群配置和使用有个大概的最佳实践
1. 尽量将多的单次请求合并为bulk 操作
2. 对实时性要求不高的索引refresh_interval调整为30s
3. 对文档丢失的关注度不高，或者有定期补偿机制的可以将index.translog.sync_interval 设置为60S index.translog.durability设置为async，以及索引不多的情况下将index.translog.flush_threshold_size调整为1GB
4. merge.policy.max_merged_segment调整为1GB，降低大数据segment参与merge segment操作
5. 不要对_id进行排序或者聚合操作
6. 不需要分词的字段不要设置为text，不必要的字段可以不需要索引
7. 减少delete_by_query的使用
8. 业务代码对ES频繁写操作要进行限流削峰
9. number_of_replica设置为1即可，2备份会消耗过多内存
10. 主分片要尽量分布在不同节点。

#force merge
## REST API
REST API，POST请求执行force_merge操作。语法如下：
curl -X POST "http://localhost:9200/my_index/_forcemerge?only_expunge_deletes=true&max_num_segments=N"
这里添加了两个参数：
only_expunge_deletes=true：仅合并包含已删除文档的段，这样可以更快地释放空间，同时对正在使用的索引影响较小。
max_num_segments=N：设置期望的合并后段数。根据实际情况选择一个合适的值，如N=5或N=10，避免设置过小导致单个段过大。单个shard对应的segment数。
##注意事项
资源消耗：force_merge操作可能会占用大量CPU、内存和磁盘I/O资源，尤其是在处理大型索引时。在执行此操作前，应评估其对集群的影响，并在低峰时段执行，或者确保有足够的资源应对。

数据可用性：在合并过程中，索引可能暂时不可搜索或不可更新。对于生产环境，建议先将索引设置为只读模式，或者创建索引副本，以确保数据的可用性和完整性。

滚动重启：对于大型索引，可能需要进行滚动重启（rolling restart）以释放文件句柄（file handles）。在某些操作系统中，单个进程打开的文件数量有限制，合并大量段可能会超过这个限制。滚动重启可以避免这个问题。

自动合并：Elasticsearch默认有一个后台进程定期合并段。如果主要关注磁盘空间优化，而搜索性能影响不大，可以调整索引的index.merge.policy设置，让Elasticsearch更积极地自动合并段，而不是频繁手动执行force_merge。

总之，force_merge命令是Elasticsearch中用于优化索引段结构的重要工具，但在使用时需谨慎考虑其对资源消耗、数据可用性等方面的影响，并根据实际需求合理设置参数。
##调整merge参数
修改索引的index.merge.policy相关参数，以促使Elasticsearch更积极地自动合并段。主要涉及以下几个参数：

index.merge.policy.segments_per_tier：每个层级（tier）的段数阈值。当某个层级的段数超过这个阈值时，Elasticsearch会尝试合并这些段。适当增大这个值可以让系统更早地触发合并。

index.merge.policy.tiered_segments_bytes：每个层级允许的最大段大小（以字节为单位）。调整这个值可以控制合并时的目标段大小。

index.merge.policy.floor_segment_bytes：合并后新段的最小大小。确保这个值足够大，可以减少小段的数量，但要避免过大导致单个段过于庞大。

index.merge.policy.max_merged_segment_bytes：合并后新段的最大大小。设置合理的上限，防止单个段过大影响搜索性能。

index.merge.scheduler.max_thread_count：合并线程池的最大线程数。增加线程数可以加速合并过程，但要注意不要过度消耗系统资源。

##segment多大合适？
Elasticsearch (ES) 中单个 segment 的大小并没有严格固定的“最合适”值，因为它取决于多种因素，包括但不限于以下几点：

硬件资源：存储容量、I/O 性能、内存大小等硬件条件直接影响了单个 segment 大小的适宜范围。在具有大量可用存储和高速 I/O 的环境中，单个 segment 可以适当增大；而在资源有限的系统中，较小的 segment 可能更为合适，以减少单个 segment 对资源的占用。

索引写入速度与频率：高写入速率或频繁更新的索引可能受益于更小的 segment，因为较小的 segment 更容易快速刷新（refresh）、合并（merge）和优化，从而更快地将新数据变为可搜索状态。反之，写入较为平稳且不需要即时查询最新数据的场景，可以允许较大的 segment。

查询性能需求：大的 segment 可能包含更多的文档，有助于提高查询效率，尤其是在进行批量扫描或聚合操作时。然而，过多的大 segment 可能增加查询时的磁盘寻道时间和内存消耗。保持适度大小的 segment 能够平衡查询效率与资源消耗。

内存管理与 GC 压力：每个 segment 都会在 Elasticsearch 的 heap 内存中保留一部分索引数据结构，用于加速查询。过多过大的 segment 可能过度占用 heap，增加 garbage collection (GC) 压力，影响整体系统稳定性。合理控制 segment 大小有助于优化内存使用，降低 GC 影响。

维护与优化操作：过大的 segment 在进行合并、搬家（relocation）等维护操作时会消耗更多的时间和资源。保持适中的 segment 大小有利于这些后台任务高效执行，避免长时间阻塞或影响服务。

Elasticsearch 配置与版本：不同的 ES 版本可能会有不同的最佳实践建议，同时，Elasticsearch 的相关配置（如 merge policy、索引设置等）也会影响 segment 的生成与管理。

考虑到以上因素，通常情况下，单个 segment 的大小可能在几兆字节（MB）到几百兆字节（MB）之间。具体到一个合适的数值，需要根据实际业务需求、硬件环境、以及通过监控和性能测试来确定。以下是一些通用的指导原则：

避免极小的 segment：过小的 segment 会导致大量的文件碎片，增加磁盘 I/O 和文件系统开销，同时也会在内存中积累大量索引数据结构，造成内存压力。

限制最大 segment 大小：通过调整 index.merge.policy.max_merge_segment 设置（在较新版本中可能为 index.merge.max_merged_segment），可以限制单个 segment 合并后的最大大小，防止其无限制增长。一般建议设置一个合理的上限，如不超过 5-20 GB。

监控与调优：持续监控 Elasticsearch 的性能指标（如查询响应时间、CPU 使用率、内存使用情况、磁盘 I/O 等），观察 segment 的数量和大小分布，根据实际情况调整索引设置或写入策略。

综上所述，确定 Elasticsearch 单个 segment 的合适大小是一个需要综合考虑多种因素并结合实际环境进行调整的过程，没有固定的“最佳”值。通过监控、测试和适时调整相关配置，可以找到符合特定业务场景的最佳折衷点。